<!DOCTYPE html>
<html lang="en">

<head>
    <meta charset="utf-8">
  <meta http-equiv="Content-Type" content="text/html" charset="UTF-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />


  <title>Assigning Confidences to LLM Outputs</title>


  <meta name="HandheldFriendly" content="True" />
  <meta name="viewport" content="width=device-width, initial-scale=1.0" />
  <meta name="referrer" content="origin" />
  <meta name="generator" content="Pelican" />
<link href="/probability-1-20250715.html" rel="canonical" />
  <!-- Feed -->

  <link rel="stylesheet" type="text/css" href="/theme/css/style.css">

  <!-- CSS specified by the user -->


  <link href="/theme/css/custom.css" type="text/css" rel="stylesheet" />

  <!-- Custom fonts -->
  <link href='https://fonts.googleapis.com/css?family=Montserrat:400,300' rel='stylesheet' type='text/css' />
  <link href="https://fonts.googleapis.com/css?family=Lato" rel="stylesheet" type="text/css" />

  <!-- HTML5 Shim and Respond.js IE8 support of HTML5 elements and media queries -->
  <!-- WARNING: Respond.js doesn't work if you view the page via file:// -->
  <!--[if lt IE 9]>
    <script src="https://oss.maxcdn.com/libs/html5shiv/3.7.0/html5shiv.js"></script>
    <script src="https://oss.maxcdn.com/libs/respond.js/1.4.2/respond.min.js"></script>
  <![endif]-->

  <script>
    var siteUrl = '';
  </script>

  <script>
    var localTheme = localStorage.getItem('attila_theme');
    switch (localTheme) {
      case 'dark':
        document.documentElement.classList.add('theme-dark');
        break;
      case 'light':
        document.documentElement.classList.add('theme-light');
        break;
      default:
        break;
    }
  </script>



<script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
<script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
<script>
  MathJax = {
    tex: {
      inlineMath: [['$', '$'], ['\\(', '\\)']],
      displayMath: [['$$', '$$'], ['\\[', '\\]']]
    }
  };
</script>



    <meta name="description" content="This is not the first time that I am encountering decisioning in the presence of uncertainty. At TruU we use various techniques (IP...">

    <meta name="author" content="Amit Agrawal">

    <meta name="tags" content="Probability">




<!-- Open Graph -->
<meta prefix="og: http://ogp.me/ns#" property="og:site_name" content="Play Deliberately"/>
<meta prefix="og: http://ogp.me/ns#" property="og:title" content="Assigning Confidences to LLM Outputs"/>
<meta prefix="og: http://ogp.me/ns#" property="og:description" content="This is not the first time that I am encountering decisioning in the presence of uncertainty. At TruU we use various techniques (IP..."/>
<meta prefix="og: http://ogp.me/ns#" property="og:locale" content="en_US"/>
<meta prefix="og: http://ogp.me/ns#" property="og:url" content="/probability-1-20250715.html"/>
<meta prefix="og: http://ogp.me/ns#" property="og:type" content="article"/>
<meta prefix="og: http://ogp.me/ns#" property="article:published_time" content="2025-07-15 00:00:00-06:00"/>
<meta prefix="og: http://ogp.me/ns#" property="article:modified_time" content=""/>
<meta prefix="og: http://ogp.me/ns#" property="article:author" content="/author/amit-agrawal.html">
<meta prefix="og: http://ogp.me/ns#" property="article:section" content="Technical"/>
<meta prefix="og: http://ogp.me/ns#" property="article:tag" content="Probability"/>
<meta prefix="og: http://ogp.me/ns#" property="og:image" content="/images/technical/LLM_uncertainty.png">

<!-- Twitter Card -->

<script type="application/ld+json">
{
  "@context": "http://schema.org",
  "@type": "Article",
  "name": "Assigning Confidences to LLM Outputs",
  "headline": "Assigning Confidences to LLM Outputs",
  "datePublished": "2025-07-15 00:00:00-06:00",
  "dateModified": "",
  "author": {
    "@type": "Person",
    "name": "Amit Agrawal",
    "url": "/author/amit-agrawal.html"
  },
  "image": "",
  "url": "/probability-1-20250715.html",
  "description": "This is not the first time that I am encountering decisioning in the presence of uncertainty. At TruU we use various techniques (IP..."
}
</script></head>









<body class="category-template">

<div class="nav-header">
  <nav class="nav-wrapper" aria-label="Main">
<ul>

    <li class="nav-Blog " role="presentation"><a href="/"><span>Blog</span></a></li>
    <li class="nav-Technical active" role="presentation"><a href="/category/technical.html"><span>Technical</span></a></li>
    <li class="nav-Business " role="presentation"><a href="/category/business.html"><span>Business</span></a></li>
    <li class="nav-Creative " role="presentation"><a href="/category/creative.html"><span>Creative</span></a></li>

      <li role="presentation"><a href="/pages/about.html"><span>About</span></a></li>
</ul>
<ul class="nav-meta">
  <li class="nav-search" style="display: none;">
    <a title="Search">
      <i class="icon icon-search" aria-hidden="true"></i>
      <span>Search</span>
    </a>
  </li>
</ul>  </nav>

  <div class="nav-wrapper-control">
    <div class="inner">
      <a class="nav-menu" role="button"><i class="icon icon-menu" aria-hidden="true"></i>Menu</a>
      <a class="nav-search" title="Search" role="button" style="display: none;"><i class="icon icon-search" aria-hidden="true"></i></a>
    </div>
  </div>
</div>
<div class="nav-close" role="button" aria-label="Close"></div>
  <section id="wrapper" class="page-wrapper">
    <!-- Progressbar -->
    <div class="progress-container">
      <span class="progress-bar"></span>
    </div>

    <!-- Page Header -->
    <!-- Set your background image for this header on the line below. -->
    <header class="post-header  has-cover ">
      <div class="inner">
        <span class="post-info">
          <span class="post-type">Article</span>
          <span class="post-count">Technical</span>
        </span>
        <h1 class="post-title">Assigning Confidences to LLM Outputs</h1>
        <div class="post-meta">
          <div class="post-meta-avatars">


            <figure class="post-meta-avatar avatar">
              <a class="author-avatar" href="/author/amit-agrawal.html">
                <img class="author-profile-image" src="" alt="Amit Agrawal" />
              </a>
            </figure>
          </div>

          <h4 class="post-meta-author">
            Amit Agrawal
          </h4>
          <time datetime="Tue 15 July 2025">Tue 15 July 2025</time>
        </div>
          <div class="post-cover cover">
              <img src="/images/technical/LLM_uncertainty.png" alt="Category Technical" />
          </div>
      </div>
    </header>

    <!-- Post content -->
    <main class="content" role="main">
        <article class="post">
        <div class="inner">
          <section class="post-content">
            <p>This is not the first time that I am encountering decisioning in the presence of uncertainty. At TruU we use various techniques (IP protected) to make reliable decisions in the presence of errors in Deep and traditional ML models. In addition, my PhD thesis involved making reliable decisions in the presence of floating point errors. </p>
<p>LLM inferences are no different. Since this is language, their are errors beyond simple correctness to account for; something I encountered in our translation tools at TruU, where we had to grapple with a nuanced notion of correctness that also involved tone, style, and cultural appropriateness in addition to semantic correctness. For this article, I will focus only on semantic correctness. This article is primarily inspired by [Liu et al. 2025, &amp; Shorinwa et al. 2025] before diving into the rabbit hole. </p>
<p>This is an updated version of the article published a few months ago. </p>
<h2>§1 White vs Black Box Approaches</h2>
<p>Appraches differ if the internals of the LLMs are available for us or not. </p>
<h3>§1.1 White Box Approaches</h3>
<ul>
<li>
<p><strong>Average Negative Token Log-Probability</strong> look at internal state of the model to compute the probabilities. For classifiers, this could just be looking at the weights of the final layer and normalizing, for generators this could mean taking the logit of each token, and computing a mean for all the tokens in the output[Duan et al 2023, Kuhn et al. 2023]. If <span class="math">\(p_{ij}\)</span> is the "weight" of the <span class="math">\(j\)</span>'th token for the <span class="math">\(i\)</span>th sentence, then the average confidence of the sentence is given by <div class="math">$$ \text{Average}(p_i) = -\frac{1}{L_i} \sum_j \log(p_{ij}) $$</div>  where <span class="math">\(L_i\)</span> is the number of tokens in the sentence.  Other statistical metrics can be used as well. So, if <span class="math">\(\text{Average}(p_i)\)</span> is small, the model is more certain. </p>
</li>
<li>
<p><strong>Entropy</strong> associated with the output distribution of token <span class="math">\(j\)</span> in sentence <span class="math">\(i\)</span> is defined as 
<div class="math">$$\mathcal{H}_{ij} = -\sum_{w \in \mathcal{D}} p_{ij}(w) \log (p_{ij})(w)$$</div> where <span class="math">\(\mathcal{D}\)</span>  denotes the dictionary containing all possible words in the model and <span class="math">\(w\)</span> represents a word in <span class="math">\(\mathcal{D}\)</span>.</p>
</li>
<li>
<p><strong>Perplexity</strong>  is the exponent of the <strong>average negative token log-probability</strong>. It is preferred by some researchers as it is more interpretable compared to negative log-prob. <div class="math">$$ \text{Perplexity}(p) = 2^{-\frac{1}{L_i} \sum_j \log(p_{ij}))} $$</div> A perplexity of <span class="math">\(k\)</span> intuitively means that a model has <span class="math">\(k\)</span> choices to choose from, whereas an <strong>average negative log-probability</strong> has no meaning except lower is better. </p>
</li>
<li>
<p><strong>Models on Internal data of LLM</strong> [Ren et al. 2022]  For LLMs with available input and output embeddings, this method trains a logistic regression classifier on concatenated question-answer embedding pairs labeled with correctness to produce a confidence function that can estimate the probability of correctness for any new LLM-generated answer given its question. </p>
</li>
</ul>
<h3>§1.2 Black Box Approaches</h3>
<p>These approaches receive greater coverage due to their broader practical applicability. A lot of commercially available LLMs are closed sourced so producing some level of confidence on their outputs continues to be an important target.</p>
<ul>
<li><strong>Self-Verbalized Uncertainty</strong> A lot of these ideas fall into the category of asking LLMs to reflect on their work. This can be as simple as asking LLM compute the confidence of its output, to different ways in which we can have LLMs self reflect on the results. There are many variations of this approach. <ul>
<li>[Wagner et al. 2024] To evaluate an answer, one can ask a question and generate features by assuming the answer is correct or incorrect, creating various rationales for both cases, then asking a language model to predict the probability of correctness given the rationale (and vice versa), using these predictions to build a confusion matrix and estimate confidence—an approach that, while demonstrated for binary classification, can also be extended to multi-class questions.</li>
<li>[Becker and Soatto 2024] An interesting variant that goes a little deeper. Generate explanations for each of the answers. Figure out entailment probability of each of the explanations. Figure out the distribution of the answers given the explanation and then marginalize the explanations given an answer. My mathematical bent appreciates this approach but there is no way to say whether this is any better than any of the simpler ones. </li>
</ul>
</li>
<li><strong>Semantic-Similarity Uncertainty</strong> Ask LLM the same question multiple times using different random seeds or temperature settings. At that point we can go multiple routes. There is<ul>
<li><strong>Graph Based: Pairwise Connections</strong> Use Natural Language Inference (NLI) models to score entailment and contradiction between every pair of generated outputs. Create an adjacency matrix of this graph. Now one can bring the entire machinary of graph theory to bear on this problem. For example, we can look at Eccentricity of the graph. For a node <span class="math">\(v\)</span> in the graph <span class="math">\(\mathcal{G}\)</span>,  <span class="math">\(\text{ecc}(v)\)</span> is the maximum <strong>shortest path</strong> distance from <span class="math">\(v\)</span> to any other node. <span class="math">\(\text{ecc}(v) = \max\limits_{u \in V} \text{dist}(v, u)\)</span>. High eccentricity → wide semantic variability. <span class="math">\(\text{radius}(\mathcal{G}) = \min\limits_{v \in V} \text{ecc}(v)\)</span> and <span class="math">\(\text{diameter}(\mathcal{G}) = \max\limits_{v \in V} \text{ecc}(v)\)</span>. By computing <span class="math">\(\text{radius}\)</span> and <span class="math">\(\text{diameter}\)</span> one can make various claims about the uncertainty in the result. In addition, we could compute spectral entropy, eigenvalue spread, and others. </li>
<li><strong>Graph Based: Kernel Language Entropy</strong>. In this approach an embedding space is chosen where the outputs are mapped. If <span class="math">\(y_i\)</span> are the various outputs from the question, then let <span class="math">\(e_i\)</span> be the corresponding embeddings. Then <div class="math">$$K(e_i, e_j) = \exp\left(-\frac{\|e_i - e_j\|^2}{2\sigma^2}\right)$$</div> can be used to create an <span class="math">\(n \times n\)</span> similarity matrix <span class="math">\(K\)</span>, that can tell how semantically each output is to othes. Given this matrix we can compute either the spectral entropy or the Renyi entropy. Renyi entropy (<span class="math">\(H_{KLE}\)</span> shown below) measures the geometric dispersion of the responses is better for this graph, spectral entropy which measures the connectedness of the graph is better suited for the connection graph shown earlier. </li>
</ul>
</li>
</ul>
<div class="math">$$H_{KLE} = -\log\left(\frac{1}{n^2} \sum_{i=1}^{n} \sum_{j=1}^{n} K(e_i, e_j)\right)$$</div>
<ul>
<li>
<p><strong>Reasoning Uncertainty</strong> Extend any of the previous approaches to Reasoning steps. For example in a Chain-of-Thought approach, measure uncertainty for each steps and then combine it, giving us CoT-UQ [Zhang et al. 2025]. Similarly Tree-of-Thought (ToT) can be extended to Tree-of-uncertain-Thought (ToUT) by computing uncertainties at each of the decision points [Mo et al. 2024]. This allows for backtracking when uncertainty is high. </p>
</li>
<li>
<p><strong>Conformal Prediction</strong> Assume that you have a calibration set that follows the distribution of the population. If this assumption is met, this approach can deliver guaranteed results. So, <span class="math">\(\mathcal{D}_{\text{cal}} = \{(x_i, y_i)\}_{i=1}^n\)</span> where <span class="math">\(x_i\)</span> is an input (e.g. a prompt) and <span class="math">\(y_i\)</span> is its true label. Define a nonconformity score <span class="math">\(S(x, \hat{y}, y)\)</span> that tells you: how bad is the model's prediction <span class="math">\(\hat{y}\)</span>, given the true answer <span class="math">\(y\)</span>. For a simple classification example this could be <span class="math">\(S = 1 - P(\hat{y})\)</span>. Now we compute the threshold <span class="math">\(\tau\)</span>, such that the error is guaranteed to be less than <span class="math">\(\alpha\)</span>. From the calibration set compute <span class="math">\(s_i = S(x_i, \hat{y}_i, y_i) \text{ for } i = 1,\dots,n\)</span>. Then compute the <span class="math">\((1-\alpha)\)</span>-quantile of the scores: <span class="math">\(\tau = \text{Quantile}_{1-\alpha}(s_i,\dots,s_n)\)</span>. This is the threshold for acceptable non-conformity. Given a new input <span class="math">\(x_{new}\)</span>, create multiple outputs <span class="math">\(\hat{y}_1,\dots,\hat{y}_k\)</span>. Select the predictions with <span class="math">\(S(x_{new},\hat{y}_j) \leq \tau\)</span>. These are guaranteed to be correct with a probabilty <span class="math">\(\geq 1 - \alpha\)</span>. [Su, Jiayuan, et al. 2024]. This approach can easily be extended for a white-box implementation so <span class="math">\(S\)</span> can take advantage of knowledge of the internal state of the LLM. </p>
</li>
</ul>
<h2>§2 Mechanistic Interpretability</h2>
<p>Even though mechanistic interpretibility is a white box technique it deserves its own section because its perpose is to truly understand how LLM understands the world and perhaps the ability to interpret the results. Some concepts from this approach</p>
<ul>
<li><strong>Superposition, polysemanticity, and circular representations</strong> Neural networks typically represent features in a straightforward manner where an n-dimensional space can encode n unique features—for instance, when the concept "Golden Gate" is present, a specific set of neurons will activate. However, LLM neural networks exhibit a phenomenon called superposition, where they represent more features than they have dimensions, meaning an n-dimensional space can capture m unique features when m &gt; n, making interpretability significantly more challenging [Elhage et al. 2022]. The converse phenomenon is polysemanticity, where a single set of neurons can encode multiple different concepts. While these concepts may overlap, the prevailing theory suggests that features still exist as linear combinations of dimensions. Recent research has revealed even more complex representational structures, with [Engels et al. 2024] discovering features that are circularly dependent, finding that concepts like days of the week and months of the year are represented in circular patterns within the neural network's feature space.</li>
<li><strong>Probing Classifier</strong> Say we want to see if a particular hidden feature encodes the concept "animal". We send various sentences through the NN and get the values of various hidden layers. If we can train a NN that can predict an animal using some subsets of some hidden layers then we can say that those parameters encode the concept "animal" [Yonatan 2022]. </li>
<li><strong>Sparse Autoencoders (SAEs)</strong> SAEs help solve the problem of superposition and polysemanticity but training a high dimensional autoencoder so that we can disentangle the polysemanticity and unique vectors encode unique concepts. This leads to more interpretable features, can allow for some control of the LLMs. So, if there is a region of the features which encodes uncertainty we can make the LLM overconfident by turning if off. </li>
</ul>
<h2>§3  Thoughts</h2>
<p>The intellectual side of me really likes the previous section so that we can finally understand why certain decisions were made and make AI truly safe. However, the practioner side of me has more immediate concerns and looks like we have lots of choices to pick from. For the applications I am interested in, I will probably pick a variant of Conformal Prediction because it gives a guaranteed outcome and we can make appropriate decisions when the uncertainty is high. </p>
<p>The challenge though is finding a calibration set that matches the population.  </p>
<h2>§4 References</h2>
<ul>
<li>[Becker and Soatto 2024] Becker, Evan, and Stefano Soatto. "Cycles of Thought: Measuring LLM Confidence through Stable Explanations." <em>arXiv preprint arXiv:2406.03441</em> (2024).</li>
<li>[Duan et al 2023] Jinhao Duan, Hao Cheng, Shiqi Wang, Chenan Wang,  Alex Zavalny, Renjing Xu, Bhavya Kailkhura, and  Kaidi Xu. 2023. Shifting attention to relevance: Towards the uncertainty estimation of large language  models. ArXiv preprint, abs/2307.01379.</li>
<li>[Elhage et al. 2022] Elhage, Nelson, et al. "Toy models of superposition." arXiv preprint arXiv:2209.10652 (2022).</li>
<li>[Engels et al. 2024] Not All Language Model Features Are Linear. arXiv  preprint arXiv:2405.14860 (2024).  </li>
<li>[Kuhn et al. 2023] Lorenz Kuhn, Yarin Gal, and Sebastian Farquhar. 2023.  Semantic uncertainty: Linguistic invariances for un-  certainty estimation in natural language generation.  ArXiv preprint, abs/2302.09664.</li>
<li>[Li, Moxin, et al. 2024 ] Li, Moxin, et al. "Think twice before assure: Confidence estimation for large language models through reflection on multiple answers." <em>arXiv preprint arXiv:2403.09972</em> (2024).</li>
<li>[Liu et al. 2025] Liu, X., Chen, T., Da, L., Chen, C., Lin, Z., &amp; Wei, H. (2025). Uncertainty quantification and confidence calibration in large language models: A survey. arXiv preprint arXiv:2503.15850.</li>
<li>[Mo et al. 2024] Shentong Mo and Miao Xin. 2024. Tree of uncertain thoughts reasoning for  large language models. In ICASSP 2024-2024 IEEE International Conference on  Acoustics, Speech and Signal Processing (ICASSP). IEEE, 12742–12746</li>
<li>[Pedapati et al. 2024] Pedapati, Tejaswini, et al. "Large Language Model Confidence Estimation via Black-Box Access." <em>arXiv preprint arXiv:2406.04370</em> (2024).</li>
<li>[Ren et al. 2022 ] Jie Ren, Jiaming Luo, Yao Zhao, Kundan Krishna, Mo-  hammad Saleh, Balaji Lakshminarayanan, and Pe-  ter J Liu. 2022. Out-of-distribution detection and  selective generation for conditional language models.  ArXiv preprint, abs/2209.15558.</li>
<li>[Shorinwa et al. 2025] Shorinwa, O., Mei, Z., Lidard, J., Ren, A. Z., &amp; Majumdar, A. (2025). A survey on uncertainty quantification of large language models: Taxonomy, open research challenges, and future directions. ACM Computing Surveys.</li>
<li>[Shrivastava et al. 2023 ] Vaishnavi Shrivastava, Percy Liang, and Ananya Kumar.  2023. Llamas know what gpts don’t show: Surrogate  models for confidence estimation. ArXiv preprint,  abs/2311.08877.</li>
<li>[Su, Jiayuan, et al. 2024] Api is enough: Conformal prediction for large language models without logit-access." arXiv preprint arXiv:2403.01216 (2024).</li>
<li>[Wagner et al. 2024]Wagner, Nico, et al. "Black-box Uncertainty Quantification Method for LLM-as-a-Judge." <em>arXiv preprint arXiv:2410.11594</em>(2024).</li>
<li>[Yonatan 2022] Yonatan Belinkov. 2022. Probing classifiers: Promises, shortcomings, and advances. Computational Linguistics 48, 1 (2022)
Hoagy Cunningham, Aidan Ewart, Logan Riggs, Robert Huben, and Lee Sharkey. 2023. Sparse autoencoders ind highly interpretable  features in language models. arXiv preprint arXiv:2309.08600 (2023).</li>
<li>[Zhang et al. 2025] Boxuan Zhang and Ruqi Zhang. 2025. CoT-UQ: Improving Response-wise  Uncertainty Quantification in LLMs with Chain-of-Thought. arXiv preprint  arXiv:2502.17214 (2025).</li>
<li>[Zhen et al. 2023] Generating with Confidence: Uncertainty Quantification for Black-box Large Language Models. Transactions on Machine Learning Research (2023)</li>
</ul>
<script type="text/javascript">if (!document.getElementById('mathjaxscript_pelican_#%@#$@#')) {
    var align = "center",
        indent = "0em",
        linebreak = "false";

    if (false) {
        align = (screen.width < 768) ? "left" : align;
        indent = (screen.width < 768) ? "0em" : indent;
        linebreak = (screen.width < 768) ? 'true' : linebreak;
    }

    var mathjaxscript = document.createElement('script');
    mathjaxscript.id = 'mathjaxscript_pelican_#%@#$@#';
    mathjaxscript.type = 'text/javascript';
    mathjaxscript.src = 'https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.3/latest.js?config=TeX-AMS-MML_HTMLorMML';

    var configscript = document.createElement('script');
    configscript.type = 'text/x-mathjax-config';
    configscript[(window.opera ? "innerHTML" : "text")] =
        "MathJax.Hub.Config({" +
        "    config: ['MMLorHTML.js']," +
        "    TeX: { extensions: ['AMSmath.js','AMSsymbols.js','noErrors.js','noUndefined.js'], equationNumbers: { autoNumber: 'none' } }," +
        "    jax: ['input/TeX','input/MathML','output/HTML-CSS']," +
        "    extensions: ['tex2jax.js','mml2jax.js','MathMenu.js','MathZoom.js']," +
        "    displayAlign: '"+ align +"'," +
        "    displayIndent: '"+ indent +"'," +
        "    showMathMenu: true," +
        "    messageStyle: 'normal'," +
        "    tex2jax: { " +
        "        inlineMath: [ ['\\\\(','\\\\)'] ], " +
        "        displayMath: [ ['$$','$$'] ]," +
        "        processEscapes: true," +
        "        preview: 'TeX'," +
        "    }, " +
        "    'HTML-CSS': { " +
        "        availableFonts: ['STIX', 'TeX']," +
        "        preferredFont: 'STIX'," +
        "        styles: { '.MathJax_Display, .MathJax .mo, .MathJax .mi, .MathJax .mn': {color: 'inherit ! important'} }," +
        "        linebreaks: { automatic: "+ linebreak +", width: '90% container' }," +
        "    }, " +
        "}); " +
        "if ('default' !== 'default') {" +
            "MathJax.Hub.Register.StartupHook('HTML-CSS Jax Ready',function () {" +
                "var VARIANT = MathJax.OutputJax['HTML-CSS'].FONTDATA.VARIANT;" +
                "VARIANT['normal'].fonts.unshift('MathJax_default');" +
                "VARIANT['bold'].fonts.unshift('MathJax_default-bold');" +
                "VARIANT['italic'].fonts.unshift('MathJax_default-italic');" +
                "VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');" +
            "});" +
            "MathJax.Hub.Register.StartupHook('SVG Jax Ready',function () {" +
                "var VARIANT = MathJax.OutputJax.SVG.FONTDATA.VARIANT;" +
                "VARIANT['normal'].fonts.unshift('MathJax_default');" +
                "VARIANT['bold'].fonts.unshift('MathJax_default-bold');" +
                "VARIANT['italic'].fonts.unshift('MathJax_default-italic');" +
                "VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');" +
            "});" +
        "}";

    (document.body || document.getElementsByTagName('head')[0]).appendChild(configscript);
    (document.body || document.getElementsByTagName('head')[0]).appendChild(mathjaxscript);
}
</script>
          </section>


          <section class="post-footer" >
            <div class="post-share">
              <span class="post-info-label">Share</span>
              <a title="Twitter" aria-label="Twitter" class="twitter" href="https://twitter.com/share?text=Assigning Confidences to LLM Outputs&amp;url=/probability-1-20250715.html" onclick="window.open(this.href, 'twitter-share', 'width=550,height=235');return false;">
                <i class="icon icon-twitter" aria-hidden="true"></i><span class="hidden">Twitter</span>
              </a>
              <a title="Facebook" aria-label="Facebook" class="facebook" href="https://www.facebook.com/sharer/sharer.php?u=/probability-1-20250715.html" onclick="window.open(this.href, 'facebook-share','width=580,height=296');return false;">
                <i class="icon icon-facebook" aria-hidden="true"></i><span class="hidden">Facebook</span>
              </a>
              <a title="LinkedIn" aria-label="LinkedIn" class="linkedin" href="https://www.linkedin.com/shareArticle?mini=true&amp;url=/probability-1-20250715.html&amp;title=Assigning Confidences to LLM Outputs" onclick="window.open(this.href, 'linkedin-share', 'width=930,height=720');return false;">
                <i class="icon icon-linkedin" aria-hidden="true"></i><span class="hidden">LinkedIn</span>
              </a>
              <a title="Email" aria-label="Email" class="email" href="mailto:?subject=Assigning Confidences to LLM Outputs&amp;body=/probability-1-20250715.html">
                <i class="icon icon-mail" aria-hidden="true"></i><span class="hidden">Email</span>
              </a>
              <div class="clear"></div>
            </div>

            <aside class="post-tags">
<a href="/tag/probability.html">Probability</a>            </aside>

            <div class="clear"></div>


          </section>


          <aside class="post-nav">
            <div class="clear"></div>
          </aside>

        </div>
      </article>
    </main>
    <div class="nav-footer">
      <nav class="nav-wrapper" aria-label="Footer">
        <span class="nav-copy">Play Deliberately &copy; 2023
        </span>
        <span class="nav-credits">



          Published with <a href="https://github.com/getpelican/pelican" rel="nofollow">Pelican</a> &bull; Theme <a href="https://github.com/arulrajnet/attila" rel="nofollow">Attila</a> &bull;
          <a class="menu-item js-theme" href="#" data-system="System theme" data-dark="Dark theme" data-light="Light theme">
            <span class="theme-icon"></span><span class="theme-text">System theme</span>
          </a>
        </span>
      </nav>
    </div>

  </section>

  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.6.4/jquery.slim.min.js"></script>
  <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.11.1/highlight.min.js"></script>
  <script type="text/javascript" src="/theme/js/jquery.fitvids.js"></script>
  <script type="text/javascript" src="/theme/js/script.js"></script>

  <!-- Script specified by the user -->
  <script type="text/javascript" src="/theme/js/custom.js"></script>
  <script type="text/javascript" src="/theme/js/related-articles.js"></script>

  <!-- 	The #block helper will pull in data from the #contentFor other template files. In this case, there's some JavaScript which we only want to use in article.html, but it needs to be included down here, after jQuery has already loaded. -->

<!-- Add MathJax and FontAwesome for Asciidoc -->\
<script>
  $(document).ready(function () {
    var viewport = $(window);
    var post = $('.post-content');
    // Responsive videos with fitVids
    post.fitVids();

    var mdSelector=".highlight pre";
    var rstSelector=".highlight pre";
    // For ":source-highlighter: highlight.js`" in asciidoc
    var adocSelector="pre.highlight > code[data-lang]"
    var selector=mdSelector;
    // Format code blocks and add line numbers
    function codestyling() {
      $(selector).each(function(i, e) {
        // Code highlight
        hljs.highlightElement(e);
        // No lines for plain text blocks
        if (!$(this).hasClass('language-text')) {
          var code = $(this);
          // Calculate amount of lines
          var lines = code.html().split(/\n(?!$)/g).length;
          var numbers = [];
          if (lines > 1) {
            lines++;
          }
          for (i = 1; i < lines; i++) {
            numbers += '<span class="line" aria-hidden="true">' + i + '</span>';
          }
          code.parent().append('<div class="lines">' + numbers + '</div>');
        }
      });
    }

    // Format code blocks only
    function codestylingWithoutLineNumbers() {
      $(selector).each(function(i, e) {
        // Code highlight
        hljs.highlightElement(e);
      });
    }

    codestylingWithoutLineNumbers();
    // Reading progress bar on window top
    function readingProgress() {
      var postBottom = post.offset().top + post.height();
      var viewportHeight = viewport.height();
      var progress = 100 - (((postBottom - (viewport.scrollTop() + viewportHeight) + viewportHeight / 3) / (postBottom - viewportHeight + viewportHeight / 3)) * 100);
      $('.progress-bar').css('width', progress + '%');
      (progress > 100) ? $('.progress-container').addClass('complete'): $('.progress-container').removeClass('complete');
    }
    readingProgress();
    // Trigger reading progress
    viewport.on({
      'scroll': function() {
        readingProgress();
      },
      'resize': function() {
        readingProgress();
      },
      'orientationchange': function() {
        readingProgress();
      }
    });

  });
</script>
</body>

</html>