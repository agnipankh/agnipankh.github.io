<!DOCTYPE html>
<html lang="en">

<head>
    <meta charset="utf-8">
  <meta http-equiv="Content-Type" content="text/html" charset="UTF-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />


  <title>Assigning Confidences to <span class="caps">LLM</span>&nbsp;Outputs</title>


  <meta name="HandheldFriendly" content="True" />
  <meta name="viewport" content="width=device-width, initial-scale=1.0" />
  <meta name="referrer" content="origin" />
  <meta name="generator" content="Pelican" />
<link href="/probability-1-20250715.html" rel="canonical" />
  <!-- Feed -->

  <link rel="stylesheet" type="text/css" href="/theme/css/style.css">

  <!-- CSS specified by the user -->


  <link href="/theme/css/custom.css" type="text/css" rel="stylesheet" />

  <!-- Custom fonts -->
  <link href='https://fonts.googleapis.com/css?family=Montserrat:400,300' rel='stylesheet' type='text/css' />
  <link href="https://fonts.googleapis.com/css?family=Lato" rel="stylesheet" type="text/css" />

  <!-- HTML5 Shim and Respond.js IE8 support of HTML5 elements and media queries -->
  <!-- WARNING: Respond.js doesn't work if you view the page via file:// -->
  <!--[if lt IE 9]>
    <script src="https://oss.maxcdn.com/libs/html5shiv/3.7.0/html5shiv.js"></script>
    <script src="https://oss.maxcdn.com/libs/respond.js/1.4.2/respond.min.js"></script>
  <![endif]-->

  <script>
    var siteUrl = '';
  </script>

  <script>
    var localTheme = localStorage.getItem('attila_theme');
    switch (localTheme) {
      case 'dark':
        document.documentElement.classList.add('theme-dark');
        break;
      case 'light':
        document.documentElement.classList.add('theme-light');
        break;
      default:
        break;
    }
  </script>



<script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
<script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
<script>
  MathJax = {
    tex: {
      inlineMath: [['$', '$'], ['\\(', '\\)']],
      displayMath: [['$$', '$$'], ['\\[', '\\]']]
    }
  };
</script>



    <meta name="description" content="Assigning Confidences to LLM Outputs This is not the first time that I am encountering decisioning in the presence of uncertainty, a...">

    <meta name="author" content="Amit Agrawal">

    <meta name="tags" content="Probability">




<!-- Open Graph -->
<meta prefix="og: http://ogp.me/ns#" property="og:site_name" content="Play Deliberately"/>
<meta prefix="og: http://ogp.me/ns#" property="og:title" content="Assigning Confidences to LLM Outputs"/>
<meta prefix="og: http://ogp.me/ns#" property="og:description" content="Assigning Confidences to LLM Outputs This is not the first time that I am encountering decisioning in the presence of uncertainty, a..."/>
<meta prefix="og: http://ogp.me/ns#" property="og:locale" content="en_US"/>
<meta prefix="og: http://ogp.me/ns#" property="og:url" content="/probability-1-20250715.html"/>
<meta prefix="og: http://ogp.me/ns#" property="og:type" content="article"/>
<meta prefix="og: http://ogp.me/ns#" property="article:published_time" content="2025-07-15 00:00:00-06:00"/>
<meta prefix="og: http://ogp.me/ns#" property="article:modified_time" content=""/>
<meta prefix="og: http://ogp.me/ns#" property="article:author" content="/author/amit-agrawal.html">
<meta prefix="og: http://ogp.me/ns#" property="article:section" content="Technical"/>
<meta prefix="og: http://ogp.me/ns#" property="article:tag" content="Probability"/>
<meta prefix="og: http://ogp.me/ns#" property="og:image" content="/images/technical/20250715_GenAIConfidence_simple_compose.png">

<!-- Twitter Card -->

<script type="application/ld+json">
{
  "@context": "http://schema.org",
  "@type": "Article",
  "name": "Assigning Confidences to LLM Outputs",
  "headline": "Assigning Confidences to LLM Outputs",
  "datePublished": "2025-07-15 00:00:00-06:00",
  "dateModified": "",
  "author": {
    "@type": "Person",
    "name": "Amit Agrawal",
    "url": "/author/amit-agrawal.html"
  },
  "image": "",
  "url": "/probability-1-20250715.html",
  "description": "Assigning Confidences to LLM Outputs This is not the first time that I am encountering decisioning in the presence of uncertainty, a..."
}
</script></head>









<body class="category-template">

<div class="nav-header">
  <nav class="nav-wrapper" aria-label="Main">
<ul>

    <li class="nav-Blog " role="presentation"><a href="/"><span>Blog</span></a></li>
    <li class="nav-Technical active" role="presentation"><a href="/category/technical.html"><span>Technical</span></a></li>
    <li class="nav-Business " role="presentation"><a href="/category/business.html"><span>Business</span></a></li>
    <li class="nav-Creative " role="presentation"><a href="/category/creative.html"><span>Creative</span></a></li>

      <li role="presentation"><a href="/pages/about.html"><span>About</span></a></li>
</ul>
<ul class="nav-meta">
  <li class="nav-search" style="display: none;">
    <a title="Search">
      <i class="icon icon-search" aria-hidden="true"></i>
      <span>Search</span>
    </a>
  </li>
</ul>  </nav>

  <div class="nav-wrapper-control">
    <div class="inner">
      <a class="nav-menu" role="button"><i class="icon icon-menu" aria-hidden="true"></i>Menu</a>
      <a class="nav-search" title="Search" role="button" style="display: none;"><i class="icon icon-search" aria-hidden="true"></i></a>
    </div>
  </div>
</div>
<div class="nav-close" role="button" aria-label="Close"></div>
  <section id="wrapper" class="page-wrapper">
    <!-- Progressbar -->
    <div class="progress-container">
      <span class="progress-bar"></span>
    </div>

    <!-- Page Header -->
    <!-- Set your background image for this header on the line below. -->
    <header class="post-header  has-cover ">
      <div class="inner">
        <span class="post-info">
          <span class="post-type">Article</span>
          <span class="post-count">Technical</span>
        </span>
        <h1 class="post-title">Assigning Confidences to <span class="caps">LLM</span>&nbsp;Outputs</h1>
        <div class="post-meta">
          <div class="post-meta-avatars">


            <figure class="post-meta-avatar avatar">
              <a class="author-avatar" href="/author/amit-agrawal.html">
                <img class="author-profile-image" src="" alt="Amit Agrawal" />
              </a>
            </figure>
          </div>

          <h4 class="post-meta-author">
            Amit Agrawal
          </h4>
          <time datetime="Tue 15 July 2025">Tue 15 July 2025</time>
        </div>
          <div class="post-cover cover">
              <img src="/images/technical/20250715_GenAIConfidence_simple_compose.png" alt="Category Technical" />
          </div>
      </div>
    </header>

    <!-- Post content -->
    <main class="content" role="main">
        <article class="post">
        <div class="inner">
          <section class="post-content">
            <h1>Assigning Confidences to <span class="caps">LLM</span>&nbsp;Outputs</h1>
<p>This is not the first time that I am encountering decisioning in the presence of uncertainty, a long time ago my PhD thesis involved making reliable decisions in the presence of floating point errors. At TruU we use various techniques (<span class="caps">IP</span> protected) to make reliable decisions in the presence of errors in Deep and traditional <span class="caps">ML</span>&nbsp;models. </p>
<p><span class="caps">LLM</span> inferences are no different. Since this is language, their are errors beyond simple correctness to account for. Something we encounterd in our translation tools at TruU, where we had to grapple with correctness as well as tone, style, cultural appropriateness. For this article we will focus only on semantic&nbsp;correctness. </p>
<p>This is an updated version of the article published a few months&nbsp;ago. </p>
<h2>Terminology</h2>
<h4>Aleatoric&nbsp;uncertainty</h4>
<p>Aleatoric uncertainty encompasses the lack of definiteness of the outcome of an event due to the inherent randomness in the process which determines the outcome of the&nbsp;event.</p>
<p>One way to think about Aleatoric Uncertainty is that one can&#8217;t reduce this uncertainty by giving more data to the model to train&nbsp;on. </p>
<h4>Epistemic&nbsp;uncertainty.</h4>
<p>Epistemic uncertainty characterizes the doubt associated with a certain outcome (prediction) due to a lack of knowledge or &#8220;ignorance&#8221; by a&nbsp;model. </p>
<p>Epistemic uncertainty can be reduced by providing more knowledge to the&nbsp;model. </p>
<h2>§1 White vs Block Box&nbsp;Approaches</h2>
<p>We can apply different approaches if the internals of the LLMs are available for&nbsp;us. </p>
<p>This is inline math: $E =&nbsp;mc^2$</p>
<p>This is display&nbsp;math:</p>
<p>$$
\int_{a}^{b} f(x) dx&nbsp;$$</p>
<h3>§1.1 White Box&nbsp;Approaches</h3>
<ul>
<li>Average Token Log-Probability. These approaches look at internal state of the model to compute the probabilities. For classifiers, this could just be looking at the weights of the final layer and normalizing, for generators this could mean taking the logit of each token, and computing a statistical metric (mean, max, median) for all the tokens in the output.[<strong>Duan et al 2023</strong>], [<strong>Kuhn et al. 2023</strong>] Even though they are pretty commonly used they are not known to be very reliable. If $p_{ij}$ is the &#8220;weight&#8221; of the $j$&#8217;th token for the $i$th sentence, then the average confidence of the sentence is given by where $L_i$ is the number of tokens in the&nbsp;sentence</li>
</ul>
<p>$$ \text{Average}(p) = -\frac{1}{L_i} \sum_j \log(p_{ij})&nbsp;$$</p>
<ul>
<li>Entropy. In the discrete case, the entropy associated with the output distribution of token $j$ in sentence $i$ is defined&nbsp;as </li>
</ul>
<p>$$\mathcal{H}$$</p>
<p>$$\mathcal{H}<em _in="\in" _mathcal_D="\mathcal{D" w>{ij} = -\sum</em>(w)$$}} p_{ij}(w) \log&nbsp;p_{ij</p>
<p>$$\mathcal{H}<em D _in="\in" w>{ij} = \sum</em> p_i&nbsp;$$</p>
<ul>
<li>Instead of logits, one can measure entropy of the output using the internal states and use it&nbsp;similarly.</li>
<li>Using an <span class="caps">ML</span> model on Embeddings[<strong>Ren et al. 2022</strong> ]: For LLMs where embeddings for input and output were present. Given a training set with a set of \&lt;Question, Answer, True/False &gt;<ul>
<li>Compute the features. Given a training set of tuples \&lt; embedding(Q), embedding(A), T/F&gt; concatenate the two embeddings, so we have the following \&lt; n-dimensional point,&nbsp;T/F&gt;.</li>
<li>Run a logistic regression on this data such that Function(Q,A) = probability of&nbsp;correctness</li>
<li>When the <span class="caps">LLM</span> produces an answer A1 given a question Q1, we can use the logistic regression to compute a confidence in the result.
So, the idea being that if an <span class="caps">LLM</span> has good knowledge about a certain subset of the embedding space, then it will continue to have correct knowledge about that&nbsp;space.</li>
</ul>
</li>
</ul>
<h3>Block Box&nbsp;Approaches</h3>
<p>These are the bigger subset of the research as these approaches have wider applicability. A lot of commercially available LLMs are closed sourced so producing some level of confidence on their outputs continues to be an important&nbsp;target.</p>
<ul>
<li>Make <span class="caps">LLM</span> reflect on its ideas. A lot of these ideas fall into the category of asking LLMs to reflect on their work. This can be as simple as asking <span class="caps">LLM</span> compute the confidence of its output to different ways in which we can have LLMs self reflect on the results.<ul>
<li>[<strong>Wagner et al. 2024</strong>]<ul>
<li>Ask a question, get the&nbsp;answer</li>
<li>Generate features<ul>
<li>assume the answer is&nbsp;correct</li>
<li>generate various rationales for the&nbsp;answer</li>
<li>assume the answer is&nbsp;incorrect</li>
<li>generate various rationales for the answer being&nbsp;wrong</li>
</ul>
</li>
<li>Ask the <span class="caps">LLM</span> to predict the probability of the answer to be correct given the rationale and vice&nbsp;versa.</li>
<li>Use this to create a confusion matrix and then come up with the confidence.
Even though this is provided only for a binary classifier, the same approach can be extended to a multi-class kind of&nbsp;questions</li>
</ul>
</li>
<li>[<strong>Shrivastava et al. 2023</strong>] This approach uses an White Box approach by picking a surrogate model e.g. Llama to generate the confidence of the generated by the <span class="caps">GPT</span>. When Llama may not generate the same answer they may either not return the result or have an ensemble of open source models to produce the result. To me this approach doesn&#8217;t sound credible. Maybe I dont fully understand how it&nbsp;works.</li>
<li>[<strong>Li, Moxin, et al. 2024 </strong>] Generate multiple answers by prompting <span class="caps">LLM</span> to  generate say 5 answers, have it generate multiple justifications for each answer. Prompt the <span class="caps">LLM</span> with the question, all the answers, and all the justifications  and have it come up with probabilities for each answer. Rerun it again and again by shuffling the order of the justifications etc. Eventually take the average of the resulting probabilities. The authors mention that shuffling of justifications was specially&nbsp;important.</li>
<li>[<strong>Becker and Soatto 2024</strong>] Another variant. Generate explanations for each of the answers. Figure out entailment probability of each of the explanations. Figure out the distribution of the answers given the explanation and then marginalize the explanations given an answer. My mathematical bent appreciates this approach but there is no way to say whether this is any better than the previous&nbsp;approach</li>
<li>[<strong>Pedapati et al. 2024</strong>] I like this approach but this is strictly a emotional bias with not much of a scientific basis for my preference. Get a data set which has the Question and Answers. Pertub the questions by various means to generate 100s of questions. These will generate lots of different answers. For each of these question answer sessions compute the 3 features researched:  (a) semantic set of the outputs, (b) lexical similarities of the outputs (rouge score), and (c) <span class="caps">SRC</span> minimum value. Now we know whether the answer generated was correct or not, so given these features and the correctness of answer we can create a logistic regression. When attempting an answer in the live setting we once again compute these 3 features and predict the confidence in the result. The fundamental insight is that these features capture the knowledge and the correctness of the knowledge of the <span class="caps">LLM</span>.</li>
</ul>
</li>
</ul>
<h1>Thoughts?</h1>
<p>At this point it is not clear whether there is a clear winner. What I have found in practice are the following best&nbsp;practices:</p>
<ul>
<li>Use multiple models from different families, and use their&nbsp;ensemble.</li>
<li>Rather than attempting to give a high fidelity answer, give an answer when u are sure but choose not to give an answer whenever in doubt. So, e.g. if you are using google, OpenAI and Anthropic as your models, give an answer with a lot of certainty when all three models agree and not give any answer when there is any&nbsp;disagreement.</li>
<li>Use an ensemble of the above techniques rather than any single&nbsp;one.</li>
<li>I believe an approach like [<strong>Pedapati et al. 2024</strong> ] which creates some features on the structure of knowledge that the <span class="caps">LLM</span> has with some <span class="caps">RLHF</span> may eventually prove to be the best but this is just a hypothesis at this&nbsp;time.</li>
</ul>
<p>Still waiting to find some research that blows me away. I am sure there are 10 more papers in this space  between me doing this research and writing this entry and perhaps a 100 more by the time you read&nbsp;it.</p>
<h1>Reference</h1>
<ul>
<li>[<strong>Becker and Soatto 2024</strong>] Becker, Evan, and Stefano Soatto. &#8220;Cycles of Thought: Measuring <span class="caps">LLM</span> Confidence through Stable Explanations.&#8221; <em>arXiv preprint arXiv:2406.03441</em>&nbsp;(2024).</li>
<li>[<strong>Duan et al 2023</strong>] Jinhao Duan, Hao Cheng, Shiqi Wang, Chenan Wang,  Alex Zavalny, Renjing Xu, Bhavya Kailkhura, and  Kaidi Xu. 2023. Shifting attention to relevance: Towards the uncertainty estimation of large language  models. ArXiv preprint,&nbsp;abs/2307.01379.</li>
<li>[<strong>Kuhn et al. 2023</strong>] Lorenz Kuhn, Yarin Gal, and Sebastian Farquhar. 2023.  Semantic uncertainty: Linguistic invariances for un-  certainty estimation in natural language generation.  ArXiv preprint,&nbsp;abs/2302.09664.</li>
<li>[<strong>Li, Moxin, et al. 2024 </strong>] Li, Moxin, et al. &#8220;Think twice before assure: Confidence estimation for large language models through reflection on multiple answers.&#8221; <em>arXiv preprint arXiv:2403.09972</em>&nbsp;(2024).</li>
<li>[<strong>Pedapati et al. 2024</strong>] Pedapati, Tejaswini, et al. &#8220;Large Language Model Confidence Estimation via Black-Box Access.&#8221; <em>arXiv preprint arXiv:2406.04370</em>&nbsp;(2024).</li>
<li>[<strong>Ren et al. 2022</strong> ] Jie Ren, Jiaming Luo, Yao Zhao, Kundan Krishna, Mo-  hammad Saleh, Balaji Lakshminarayanan, and Pe-  ter J Liu. 2022. Out-of-distribution detection and  selective generation for conditional language models.  ArXiv preprint,&nbsp;abs/2209.15558.</li>
<li>
<p>[<strong>Shrivastava et al. 2023 </strong>] Vaishnavi Shrivastava, Percy Liang, and Ananya Kumar.  2023. Llamas know what gpts don’t show: Surrogate  models for confidence estimation. ArXiv preprint,&nbsp;abs/2311.08877.</p>
</li>
<li>
<p>[<strong>Wagner et al. 2024</strong>]Wagner, Nico, et al. &#8220;Black-box Uncertainty Quantification Method for <span class="caps">LLM</span>-as-a-Judge.&#8221; <em>arXiv preprint arXiv:2410.11594</em>(2024).</p>
</li>
</ul>
          </section>


          <section class="post-footer" >
            <div class="post-share">
              <span class="post-info-label">Share</span>
              <a title="Twitter" aria-label="Twitter" class="twitter" href="https://twitter.com/share?text=Assigning Confidences to <span class="caps">LLM</span>&nbsp;Outputs&amp;url=/probability-1-20250715.html" onclick="window.open(this.href, 'twitter-share', 'width=550,height=235');return false;">
                <i class="icon icon-twitter" aria-hidden="true"></i><span class="hidden">Twitter</span>
              </a>
              <a title="Facebook" aria-label="Facebook" class="facebook" href="https://www.facebook.com/sharer/sharer.php?u=/probability-1-20250715.html" onclick="window.open(this.href, 'facebook-share','width=580,height=296');return false;">
                <i class="icon icon-facebook" aria-hidden="true"></i><span class="hidden">Facebook</span>
              </a>
              <a title="LinkedIn" aria-label="LinkedIn" class="linkedin" href="https://www.linkedin.com/shareArticle?mini=true&amp;url=/probability-1-20250715.html&amp;title=Assigning Confidences to <span class="caps">LLM</span>&nbsp;Outputs" onclick="window.open(this.href, 'linkedin-share', 'width=930,height=720');return false;">
                <i class="icon icon-linkedin" aria-hidden="true"></i><span class="hidden">LinkedIn</span>
              </a>
              <a title="Email" aria-label="Email" class="email" href="mailto:?subject=Assigning Confidences to <span class="caps">LLM</span>&nbsp;Outputs&amp;body=/probability-1-20250715.html">
                <i class="icon icon-mail" aria-hidden="true"></i><span class="hidden">Email</span>
              </a>
              <div class="clear"></div>
            </div>

            <aside class="post-tags">
<a href="/tag/probability.html">Probability</a>            </aside>

            <div class="clear"></div>


          </section>


          <aside class="post-nav">
            <div class="clear"></div>
          </aside>

        </div>
      </article>
    </main>
    <div class="nav-footer">
      <nav class="nav-wrapper" aria-label="Footer">
        <span class="nav-copy">Play Deliberately &copy; 2023
        </span>
        <span class="nav-credits">



          Published with <a href="https://github.com/getpelican/pelican" rel="nofollow">Pelican</a> &bull; Theme <a href="https://github.com/arulrajnet/attila" rel="nofollow">Attila</a> &bull;
          <a class="menu-item js-theme" href="#" data-system="System theme" data-dark="Dark theme" data-light="Light theme">
            <span class="theme-icon"></span><span class="theme-text">System theme</span>
          </a>
        </span>
      </nav>
    </div>

  </section>

  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.6.4/jquery.slim.min.js"></script>
  <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.11.1/highlight.min.js"></script>
  <script type="text/javascript" src="/theme/js/jquery.fitvids.js"></script>
  <script type="text/javascript" src="/theme/js/script.js"></script>

  <!-- Script specified by the user -->
  <script type="text/javascript" src="/theme/js/custom.js"></script>
  <script type="text/javascript" src="/theme/js/related-articles.js"></script>

  <!-- 	The #block helper will pull in data from the #contentFor other template files. In this case, there's some JavaScript which we only want to use in article.html, but it needs to be included down here, after jQuery has already loaded. -->

<!-- Add MathJax and FontAwesome for Asciidoc -->\
<script>
  $(document).ready(function () {
    var viewport = $(window);
    var post = $('.post-content');
    // Responsive videos with fitVids
    post.fitVids();

    var mdSelector=".highlight pre";
    var rstSelector=".highlight pre";
    // For ":source-highlighter: highlight.js`" in asciidoc
    var adocSelector="pre.highlight > code[data-lang]"
    var selector=mdSelector;
    // Format code blocks and add line numbers
    function codestyling() {
      $(selector).each(function(i, e) {
        // Code highlight
        hljs.highlightElement(e);
        // No lines for plain text blocks
        if (!$(this).hasClass('language-text')) {
          var code = $(this);
          // Calculate amount of lines
          var lines = code.html().split(/\n(?!$)/g).length;
          var numbers = [];
          if (lines > 1) {
            lines++;
          }
          for (i = 1; i < lines; i++) {
            numbers += '<span class="line" aria-hidden="true">' + i + '</span>';
          }
          code.parent().append('<div class="lines">' + numbers + '</div>');
        }
      });
    }

    // Format code blocks only
    function codestylingWithoutLineNumbers() {
      $(selector).each(function(i, e) {
        // Code highlight
        hljs.highlightElement(e);
      });
    }

    codestylingWithoutLineNumbers();
    // Reading progress bar on window top
    function readingProgress() {
      var postBottom = post.offset().top + post.height();
      var viewportHeight = viewport.height();
      var progress = 100 - (((postBottom - (viewport.scrollTop() + viewportHeight) + viewportHeight / 3) / (postBottom - viewportHeight + viewportHeight / 3)) * 100);
      $('.progress-bar').css('width', progress + '%');
      (progress > 100) ? $('.progress-container').addClass('complete'): $('.progress-container').removeClass('complete');
    }
    readingProgress();
    // Trigger reading progress
    viewport.on({
      'scroll': function() {
        readingProgress();
      },
      'resize': function() {
        readingProgress();
      },
      'orientationchange': function() {
        readingProgress();
      }
    });

  });
</script>
</body>

</html>