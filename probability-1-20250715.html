<!DOCTYPE html>
<html lang="en">

<head>
    <meta charset="utf-8">
  <meta http-equiv="Content-Type" content="text/html" charset="UTF-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />


  <title>Assigning Confidences to LLM Outputs</title>


  <meta name="HandheldFriendly" content="True" />
  <meta name="viewport" content="width=device-width, initial-scale=1.0" />
  <meta name="referrer" content="origin" />
  <meta name="generator" content="Pelican" />
<link href="/probability-1-20250715.html" rel="canonical" />
  <!-- Feed -->

  <link rel="stylesheet" type="text/css" href="/theme/css/style.css">

  <!-- CSS specified by the user -->


  <link href="/theme/css/custom.css" type="text/css" rel="stylesheet" />

  <!-- Custom fonts -->
  <link href='https://fonts.googleapis.com/css?family=Montserrat:400,300' rel='stylesheet' type='text/css' />
  <link href="https://fonts.googleapis.com/css?family=Lato" rel="stylesheet" type="text/css" />

  <!-- HTML5 Shim and Respond.js IE8 support of HTML5 elements and media queries -->
  <!-- WARNING: Respond.js doesn't work if you view the page via file:// -->
  <!--[if lt IE 9]>
    <script src="https://oss.maxcdn.com/libs/html5shiv/3.7.0/html5shiv.js"></script>
    <script src="https://oss.maxcdn.com/libs/respond.js/1.4.2/respond.min.js"></script>
  <![endif]-->

  <script>
    var siteUrl = '';
  </script>

  <script>
    var localTheme = localStorage.getItem('attila_theme');
    switch (localTheme) {
      case 'dark':
        document.documentElement.classList.add('theme-dark');
        break;
      case 'light':
        document.documentElement.classList.add('theme-light');
        break;
      default:
        break;
    }
  </script>



<script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
<script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
<script>
  MathJax = {
    tex: {
      inlineMath: [['$', '$'], ['\\(', '\\)']],
      displayMath: [['$$', '$$'], ['\\[', '\\]']]
    }
  };
</script>



    <meta name="description" content="Assigning Confidences to LLM Outputs This is not the first time that I am encountering decisioning in the presence of uncertainty, a...">

    <meta name="author" content="Amit Agrawal">

    <meta name="tags" content="Probability">




<!-- Open Graph -->
<meta prefix="og: http://ogp.me/ns#" property="og:site_name" content="Play Deliberately"/>
<meta prefix="og: http://ogp.me/ns#" property="og:title" content="Assigning Confidences to LLM Outputs"/>
<meta prefix="og: http://ogp.me/ns#" property="og:description" content="Assigning Confidences to LLM Outputs This is not the first time that I am encountering decisioning in the presence of uncertainty, a..."/>
<meta prefix="og: http://ogp.me/ns#" property="og:locale" content="en_US"/>
<meta prefix="og: http://ogp.me/ns#" property="og:url" content="/probability-1-20250715.html"/>
<meta prefix="og: http://ogp.me/ns#" property="og:type" content="article"/>
<meta prefix="og: http://ogp.me/ns#" property="article:published_time" content="2025-07-15 00:00:00-06:00"/>
<meta prefix="og: http://ogp.me/ns#" property="article:modified_time" content=""/>
<meta prefix="og: http://ogp.me/ns#" property="article:author" content="/author/amit-agrawal.html">
<meta prefix="og: http://ogp.me/ns#" property="article:section" content="Technical"/>
<meta prefix="og: http://ogp.me/ns#" property="article:tag" content="Probability"/>
<meta prefix="og: http://ogp.me/ns#" property="og:image" content="/images/technical/20250715_GenAIConfidence_simple_compose.png">

<!-- Twitter Card -->

<script type="application/ld+json">
{
  "@context": "http://schema.org",
  "@type": "Article",
  "name": "Assigning Confidences to LLM Outputs",
  "headline": "Assigning Confidences to LLM Outputs",
  "datePublished": "2025-07-15 00:00:00-06:00",
  "dateModified": "",
  "author": {
    "@type": "Person",
    "name": "Amit Agrawal",
    "url": "/author/amit-agrawal.html"
  },
  "image": "",
  "url": "/probability-1-20250715.html",
  "description": "Assigning Confidences to LLM Outputs This is not the first time that I am encountering decisioning in the presence of uncertainty, a..."
}
</script></head>









<body class="category-template">

<div class="nav-header">
  <nav class="nav-wrapper" aria-label="Main">
<ul>

    <li class="nav-Blog " role="presentation"><a href="/"><span>Blog</span></a></li>
    <li class="nav-Technical active" role="presentation"><a href="/category/technical.html"><span>Technical</span></a></li>
    <li class="nav-Business " role="presentation"><a href="/category/business.html"><span>Business</span></a></li>
    <li class="nav-Creative " role="presentation"><a href="/category/creative.html"><span>Creative</span></a></li>

      <li role="presentation"><a href="/pages/about.html"><span>About</span></a></li>
</ul>
<ul class="nav-meta">
  <li class="nav-search" style="display: none;">
    <a title="Search">
      <i class="icon icon-search" aria-hidden="true"></i>
      <span>Search</span>
    </a>
  </li>
</ul>  </nav>

  <div class="nav-wrapper-control">
    <div class="inner">
      <a class="nav-menu" role="button"><i class="icon icon-menu" aria-hidden="true"></i>Menu</a>
      <a class="nav-search" title="Search" role="button" style="display: none;"><i class="icon icon-search" aria-hidden="true"></i></a>
    </div>
  </div>
</div>
<div class="nav-close" role="button" aria-label="Close"></div>
  <section id="wrapper" class="page-wrapper">
    <!-- Progressbar -->
    <div class="progress-container">
      <span class="progress-bar"></span>
    </div>

    <!-- Page Header -->
    <!-- Set your background image for this header on the line below. -->
    <header class="post-header  has-cover ">
      <div class="inner">
        <span class="post-info">
          <span class="post-type">Article</span>
          <span class="post-count">Technical</span>
        </span>
        <h1 class="post-title">Assigning Confidences to LLM Outputs</h1>
        <div class="post-meta">
          <div class="post-meta-avatars">


            <figure class="post-meta-avatar avatar">
              <a class="author-avatar" href="/author/amit-agrawal.html">
                <img class="author-profile-image" src="" alt="Amit Agrawal" />
              </a>
            </figure>
          </div>

          <h4 class="post-meta-author">
            Amit Agrawal
          </h4>
          <time datetime="Tue 15 July 2025">Tue 15 July 2025</time>
        </div>
          <div class="post-cover cover">
              <img src="/images/technical/20250715_GenAIConfidence_simple_compose.png" alt="Category Technical" />
          </div>
      </div>
    </header>

    <!-- Post content -->
    <main class="content" role="main">
        <article class="post">
        <div class="inner">
          <section class="post-content">
            <h1>Assigning Confidences to LLM Outputs</h1>
<p>This is not the first time that I am encountering decisioning in the presence of uncertainty, a long time ago my PhD thesis involved making reliable decisions in the presence of floating point errors. At TruU we use various techniques (IP protected) to make reliable decisions in the presence of errors in Deep and traditional ML models. </p>
<p>LLM inferences are no different. Since this is language, their are errors beyond simple correctness to account for. Something we encounterd in our translation tools at TruU, where we had to grapple with correctness as well as tone, style, cultural appropriateness. For this article we will focus only on semantic correctness. </p>
<p>This is an updated version of the article published a few months ago. </p>
<h2>Terminology</h2>
<h4>Aleatoric uncertainty</h4>
<p>Aleatoric uncertainty encompasses the lack of definiteness of the outcome of an event due to the inherent randomness in the process which determines the outcome of the event.</p>
<p>One way to think about Aleatoric Uncertainty is that one can't reduce this uncertainty by giving more data to the model to train on. </p>
<h4>Epistemic uncertainty.</h4>
<p>Epistemic uncertainty characterizes the doubt associated with a certain outcome (prediction) due to a lack of knowledge or "ignorance" by a model. </p>
<p>Epistemic uncertainty can be reduced by providing more knowledge to the model. </p>
<h2>§1 White vs Black Box Approaches</h2>
<p>We can apply different approaches if the internals of the LLMs are available for us. </p>
<h3>§1.1 White Box Approaches</h3>
<ul>
<li><strong>Average Negative Token Log-Probability</strong> look at internal state of the model to compute the probabilities. For classifiers, this could just be looking at the weights of the final layer and normalizing, for generators this could mean taking the logit of each token, and computing a statistical metric (mean, max, median) for all the tokens in the output.[<strong>Duan et al 2023</strong>], [<strong>Kuhn et al. 2023</strong>] Even though they are pretty commonly used they are not known to be very reliable. If <span class="math">\(p_{ij}\)</span> is the "weight" of the <span class="math">\(j\)</span>'th token for the <span class="math">\(i\)</span>th sentence, then the average confidence of the sentence is given by where <span class="math">\(L_i\)</span> is the number of tokens in the sentence</li>
</ul>
<div class="math">$$ \text{Average}(p) = -\frac{1}{L_i} \sum_j \log(p_{ij}) $$</div>
<ul>
<li>
<p><strong>Entropy</strong> associated with the output distribution of token <span class="math">\(j\)</span> in sentence <span class="math">\(i\)</span> is defined as 
<div class="math">$$\mathcal{H}_{ij} = -\sum_{w \in \mathcal{D}} p_{ij}(w) \log (p_{ij})(w)$$</div> where <span class="math">\(\mathcal{D}\)</span>  denotes the dictionary containing all possible words in the model and <span class="math">\(w\)</span> represents a word in <span class="math">\(\mathcal{D}\)</span>.</p>
</li>
<li>
<p><strong>Perplexity</strong>  measures how surprised the model is by its own generated text. Its just an exponent of the average negative token log-probability. It is preferred by some researchers as it is a little bit more understandable compared to negative log-prob. 
<div class="math">$$ \text{Perplexity}(p) = e^{-\frac{1}{L_i} \sum_j \log(p_{ij}))} $$</div>
</p>
</li>
<li>
<p><strong>Regression model on Embeddings</strong> [<strong>Ren et al. 2022</strong> ]: For LLMs where embeddings for input and output were present. Given a training set with a set of question and answer tuples,  <span class="math">\((Q_i, A_i, p ) \text{ where } p \in \text{true,false}\)</span></p>
<ul>
<li>Compute the features. Given a training set of tuples <span class="math">\((embedding(Q_i), embedding(A_i),p)\)</span> concatenate the two embeddings, so we have the following <span class="math">\((P_i, p) \text{ where } P_i \in \mathcal{R^k}\)</span></li>
<li>Run a logistic regression on this data such that Function(Q,A) = probability of correctness</li>
<li>When the LLM produces an answer <span class="math">\(A_i\)</span> given a question <span class="math">\(Q_i\)</span>, we can use the logistic regression to compute a confidence in the result.</li>
</ul>
</li>
</ul>
<h3>§1.2 Black Box Approaches</h3>
<p>These are the bigger subset of the research as these approaches have wider applicability. A lot of commercially available LLMs are closed sourced so producing some level of confidence on their outputs continues to be an important target.</p>
<ul>
<li><strong>Self-Verbalized Uncertainty</strong> A lot of these ideas fall into the category of asking LLMs to reflect on their work. This can be as simple as asking LLM compute the confidence of its output to different ways in which we can have LLMs self reflect on the results. There are many variations of this approach. <ul>
<li>[<strong>Wagner et al. 2024</strong>] To evaluate an answer, one can ask a question and generate features by assuming the answer is correct or incorrect, creating various rationales for both cases, then asking a language model to predict the probability of correctness given the rationale (and vice versa), using these predictions to build a confusion matrix and estimate confidence—an approach that, while demonstrated for binary classification, can also be extended to multi-class questions.</li>
<li>[<strong>Becker and Soatto 2024</strong>] An interesting variant that goes a little deeper. Generate explanations for each of the answers. Figure out entailment probability of each of the explanations. Figure out the distribution of the answers given the explanation and then marginalize the explanations given an answer. My mathematical bent appreciates this approach but there is no way to say whether this is any better than any of the simpler ones. </li>
</ul>
</li>
<li>
<p><strong>Semantic-Similarity Uncertainty</strong> Ask LLM the same question multiple times using different random seeds or temperature settings. At that point we can go multiple routes. There is</p>
<ul>
<li>
<p><strong>Graph Based Methods</strong> Use Natural Language Inference (NLI) models to score entailment and contradiction between every pair of generated outputs. Create an adjacency matrix of this graph. Now one can bring the entire machinary of graph theory to bear on this problem. We can look at Eccentricity of the graph <div class="math">$$\mathrm{ecc}(v) = \max_{u \in V} \mathrm{dist}(v, u)$$</div>
</p>
</li>
<li>
<p>[<strong>Pedapati et al. 2024</strong>] Get a data set which has the Question and Answers. Pertub the questions by various means to generate 100s of questions. These will generate lots of different answers. For each of these question answer sessions compute these 3 features:  (a) semantic set of the outputs, (b) lexical similarities of the outputs (rouge score), and (c) SRC minimum value (Self Referenced Confidence). Now we know whether the answer generated was correct or not, so given these features and the correctness of answer we can create a logistic regression. When attempting an answer in the live setting we once again compute these 3 features and predict the confidence in the result. The fundamental insight is that these features capture the knowledge and the correctness of the knowledge of the LLM.</p>
</li>
</ul>
</li>
</ul>
<h1>Thoughts?</h1>
<p>At this point it is not clear whether there is a clear winner. What I have found in practice are the following best practices:</p>
<ul>
<li>Use multiple models from different families, and use their ensemble.</li>
<li>Rather than attempting to give a high fidelity answer, give an answer when u are sure but choose not to give an answer whenever in doubt. So, e.g. if you are using google, OpenAI and Anthropic as your models, give an answer with a lot of certainty when all three models agree and not give any answer when there is any disagreement.</li>
<li>Use an ensemble of the above techniques rather than any single one.</li>
<li>I believe an approach like [<strong>Pedapati et al. 2024</strong> ] which creates some features on the structure of knowledge that the LLM has with some RLHF may eventually prove to be the best but this is just a hypothesis at this time.</li>
</ul>
<p>Still waiting to find some research that blows me away. I am sure there are 10 more papers in this space  between me doing this research and writing this entry and perhaps a 100 more by the time you read it.</p>
<h1>Reference</h1>
<ul>
<li>[<strong>Becker and Soatto 2024</strong>] Becker, Evan, and Stefano Soatto. "Cycles of Thought: Measuring LLM Confidence through Stable Explanations." <em>arXiv preprint arXiv:2406.03441</em> (2024).</li>
<li>[<strong>Duan et al 2023</strong>] Jinhao Duan, Hao Cheng, Shiqi Wang, Chenan Wang,  Alex Zavalny, Renjing Xu, Bhavya Kailkhura, and  Kaidi Xu. 2023. Shifting attention to relevance: Towards the uncertainty estimation of large language  models. ArXiv preprint, abs/2307.01379.</li>
<li>[<strong>Kuhn et al. 2023</strong>] Lorenz Kuhn, Yarin Gal, and Sebastian Farquhar. 2023.  Semantic uncertainty: Linguistic invariances for un-  certainty estimation in natural language generation.  ArXiv preprint, abs/2302.09664.</li>
<li>[<strong>Li, Moxin, et al. 2024 </strong>] Li, Moxin, et al. "Think twice before assure: Confidence estimation for large language models through reflection on multiple answers." <em>arXiv preprint arXiv:2403.09972</em> (2024).</li>
<li>[<strong>Pedapati et al. 2024</strong>] Pedapati, Tejaswini, et al. "Large Language Model Confidence Estimation via Black-Box Access." <em>arXiv preprint arXiv:2406.04370</em> (2024).</li>
<li>[<strong>Ren et al. 2022</strong> ] Jie Ren, Jiaming Luo, Yao Zhao, Kundan Krishna, Mo-  hammad Saleh, Balaji Lakshminarayanan, and Pe-  ter J Liu. 2022. Out-of-distribution detection and  selective generation for conditional language models.  ArXiv preprint, abs/2209.15558.</li>
<li>
<p>[<strong>Shrivastava et al. 2023 </strong>] Vaishnavi Shrivastava, Percy Liang, and Ananya Kumar.  2023. Llamas know what gpts don’t show: Surrogate  models for confidence estimation. ArXiv preprint,  abs/2311.08877.</p>
</li>
<li>
<p>[<strong>Wagner et al. 2024</strong>]Wagner, Nico, et al. "Black-box Uncertainty Quantification Method for LLM-as-a-Judge." <em>arXiv preprint arXiv:2410.11594</em>(2024).</p>
</li>
<li>
<p>[<strong>Zhen et al. 2023</strong>] Generating with Confidence: Uncertainty Quantification for Black-box Large Language Models. Transactions on Machine Learning Research (2023)</p>
</li>
</ul>
<script type="text/javascript">if (!document.getElementById('mathjaxscript_pelican_#%@#$@#')) {
    var align = "center",
        indent = "0em",
        linebreak = "false";

    if (false) {
        align = (screen.width < 768) ? "left" : align;
        indent = (screen.width < 768) ? "0em" : indent;
        linebreak = (screen.width < 768) ? 'true' : linebreak;
    }

    var mathjaxscript = document.createElement('script');
    mathjaxscript.id = 'mathjaxscript_pelican_#%@#$@#';
    mathjaxscript.type = 'text/javascript';
    mathjaxscript.src = 'https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.3/latest.js?config=TeX-AMS-MML_HTMLorMML';

    var configscript = document.createElement('script');
    configscript.type = 'text/x-mathjax-config';
    configscript[(window.opera ? "innerHTML" : "text")] =
        "MathJax.Hub.Config({" +
        "    config: ['MMLorHTML.js']," +
        "    TeX: { extensions: ['AMSmath.js','AMSsymbols.js','noErrors.js','noUndefined.js'], equationNumbers: { autoNumber: 'none' } }," +
        "    jax: ['input/TeX','input/MathML','output/HTML-CSS']," +
        "    extensions: ['tex2jax.js','mml2jax.js','MathMenu.js','MathZoom.js']," +
        "    displayAlign: '"+ align +"'," +
        "    displayIndent: '"+ indent +"'," +
        "    showMathMenu: true," +
        "    messageStyle: 'normal'," +
        "    tex2jax: { " +
        "        inlineMath: [ ['\\\\(','\\\\)'] ], " +
        "        displayMath: [ ['$$','$$'] ]," +
        "        processEscapes: true," +
        "        preview: 'TeX'," +
        "    }, " +
        "    'HTML-CSS': { " +
        "        availableFonts: ['STIX', 'TeX']," +
        "        preferredFont: 'STIX'," +
        "        styles: { '.MathJax_Display, .MathJax .mo, .MathJax .mi, .MathJax .mn': {color: 'inherit ! important'} }," +
        "        linebreaks: { automatic: "+ linebreak +", width: '90% container' }," +
        "    }, " +
        "}); " +
        "if ('default' !== 'default') {" +
            "MathJax.Hub.Register.StartupHook('HTML-CSS Jax Ready',function () {" +
                "var VARIANT = MathJax.OutputJax['HTML-CSS'].FONTDATA.VARIANT;" +
                "VARIANT['normal'].fonts.unshift('MathJax_default');" +
                "VARIANT['bold'].fonts.unshift('MathJax_default-bold');" +
                "VARIANT['italic'].fonts.unshift('MathJax_default-italic');" +
                "VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');" +
            "});" +
            "MathJax.Hub.Register.StartupHook('SVG Jax Ready',function () {" +
                "var VARIANT = MathJax.OutputJax.SVG.FONTDATA.VARIANT;" +
                "VARIANT['normal'].fonts.unshift('MathJax_default');" +
                "VARIANT['bold'].fonts.unshift('MathJax_default-bold');" +
                "VARIANT['italic'].fonts.unshift('MathJax_default-italic');" +
                "VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');" +
            "});" +
        "}";

    (document.body || document.getElementsByTagName('head')[0]).appendChild(configscript);
    (document.body || document.getElementsByTagName('head')[0]).appendChild(mathjaxscript);
}
</script>
          </section>


          <section class="post-footer" >
            <div class="post-share">
              <span class="post-info-label">Share</span>
              <a title="Twitter" aria-label="Twitter" class="twitter" href="https://twitter.com/share?text=Assigning Confidences to LLM Outputs&amp;url=/probability-1-20250715.html" onclick="window.open(this.href, 'twitter-share', 'width=550,height=235');return false;">
                <i class="icon icon-twitter" aria-hidden="true"></i><span class="hidden">Twitter</span>
              </a>
              <a title="Facebook" aria-label="Facebook" class="facebook" href="https://www.facebook.com/sharer/sharer.php?u=/probability-1-20250715.html" onclick="window.open(this.href, 'facebook-share','width=580,height=296');return false;">
                <i class="icon icon-facebook" aria-hidden="true"></i><span class="hidden">Facebook</span>
              </a>
              <a title="LinkedIn" aria-label="LinkedIn" class="linkedin" href="https://www.linkedin.com/shareArticle?mini=true&amp;url=/probability-1-20250715.html&amp;title=Assigning Confidences to LLM Outputs" onclick="window.open(this.href, 'linkedin-share', 'width=930,height=720');return false;">
                <i class="icon icon-linkedin" aria-hidden="true"></i><span class="hidden">LinkedIn</span>
              </a>
              <a title="Email" aria-label="Email" class="email" href="mailto:?subject=Assigning Confidences to LLM Outputs&amp;body=/probability-1-20250715.html">
                <i class="icon icon-mail" aria-hidden="true"></i><span class="hidden">Email</span>
              </a>
              <div class="clear"></div>
            </div>

            <aside class="post-tags">
<a href="/tag/probability.html">Probability</a>            </aside>

            <div class="clear"></div>


          </section>


          <aside class="post-nav">
            <div class="clear"></div>
          </aside>

        </div>
      </article>
    </main>
    <div class="nav-footer">
      <nav class="nav-wrapper" aria-label="Footer">
        <span class="nav-copy">Play Deliberately &copy; 2023
        </span>
        <span class="nav-credits">



          Published with <a href="https://github.com/getpelican/pelican" rel="nofollow">Pelican</a> &bull; Theme <a href="https://github.com/arulrajnet/attila" rel="nofollow">Attila</a> &bull;
          <a class="menu-item js-theme" href="#" data-system="System theme" data-dark="Dark theme" data-light="Light theme">
            <span class="theme-icon"></span><span class="theme-text">System theme</span>
          </a>
        </span>
      </nav>
    </div>

  </section>

  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.6.4/jquery.slim.min.js"></script>
  <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.11.1/highlight.min.js"></script>
  <script type="text/javascript" src="/theme/js/jquery.fitvids.js"></script>
  <script type="text/javascript" src="/theme/js/script.js"></script>

  <!-- Script specified by the user -->
  <script type="text/javascript" src="/theme/js/custom.js"></script>
  <script type="text/javascript" src="/theme/js/related-articles.js"></script>

  <!-- 	The #block helper will pull in data from the #contentFor other template files. In this case, there's some JavaScript which we only want to use in article.html, but it needs to be included down here, after jQuery has already loaded. -->

<!-- Add MathJax and FontAwesome for Asciidoc -->\
<script>
  $(document).ready(function () {
    var viewport = $(window);
    var post = $('.post-content');
    // Responsive videos with fitVids
    post.fitVids();

    var mdSelector=".highlight pre";
    var rstSelector=".highlight pre";
    // For ":source-highlighter: highlight.js`" in asciidoc
    var adocSelector="pre.highlight > code[data-lang]"
    var selector=mdSelector;
    // Format code blocks and add line numbers
    function codestyling() {
      $(selector).each(function(i, e) {
        // Code highlight
        hljs.highlightElement(e);
        // No lines for plain text blocks
        if (!$(this).hasClass('language-text')) {
          var code = $(this);
          // Calculate amount of lines
          var lines = code.html().split(/\n(?!$)/g).length;
          var numbers = [];
          if (lines > 1) {
            lines++;
          }
          for (i = 1; i < lines; i++) {
            numbers += '<span class="line" aria-hidden="true">' + i + '</span>';
          }
          code.parent().append('<div class="lines">' + numbers + '</div>');
        }
      });
    }

    // Format code blocks only
    function codestylingWithoutLineNumbers() {
      $(selector).each(function(i, e) {
        // Code highlight
        hljs.highlightElement(e);
      });
    }

    codestylingWithoutLineNumbers();
    // Reading progress bar on window top
    function readingProgress() {
      var postBottom = post.offset().top + post.height();
      var viewportHeight = viewport.height();
      var progress = 100 - (((postBottom - (viewport.scrollTop() + viewportHeight) + viewportHeight / 3) / (postBottom - viewportHeight + viewportHeight / 3)) * 100);
      $('.progress-bar').css('width', progress + '%');
      (progress > 100) ? $('.progress-container').addClass('complete'): $('.progress-container').removeClass('complete');
    }
    readingProgress();
    // Trigger reading progress
    viewport.on({
      'scroll': function() {
        readingProgress();
      },
      'resize': function() {
        readingProgress();
      },
      'orientationchange': function() {
        readingProgress();
      }
    });

  });
</script>
</body>

</html>