<!DOCTYPE html>
<html lang="en">
        <head>
                        <meta charset="utf-8" />
                        <meta name="viewport" content="width=device-width, initial-scale=1.0" />
                        <meta name="generator" content="Pelican" />
                        <title>Nitty Gritty of Style Transfer</title>
                        <link rel="stylesheet" href="/theme/css/main.css" />
    <meta name="description" content="Style transfer was first introduced by Aaron Hertzmann’s classic paper on Image Analogies from SIGGRAPH 2001. More recently deep learning got..." />
        </head>

        <body id="index" class="home">
                <header id="banner" class="body">
                        <h1><a href="/">Play Deliberately</a></h1>
                        <nav><ul>
                                                <li><a href="/pages/about.html">About</a></li>
                                                <li><a href="/category/business.html">Business</a></li>
                                                <li><a href="/category/creative.html">Creative</a></li>
                                                <li class="active"><a href="/category/technical.html">Technical</a></li>
                        </ul></nav>
                </header><!-- /#banner -->
  <section id="content" class="body">
    <article>
      <header>
        <h1 class="entry-title">
          <a href="/DNN-1-20171125.html" rel="bookmark"
             title="Permalink to Nitty Gritty of Style Transfer">Nitty Gritty of Style&nbsp;Transfer</a></h1>
      </header>

      <div class="entry-content">
<footer class="post-info">
        <abbr class="published" title="2017-11-25T00:00:00-07:00">
                Published: Sat 25 November 2017
        </abbr>

                <address class="vcard author">
                        By                                 <a class="url fn" href="/author/amit-agrawal.html">Amit Agrawal</a>
                </address>
        <p>In <a href="/category/technical.html">Technical</a>.</p>
<p>tags: <a href="/tag/deep-networks.html">Deep Networks</a> </p>        
</footer><!-- /.post-info -->        <p>Style transfer was first introduced by Aaron Hertzmann&#8217;s classic paper on Image Analogies from <span class="caps">SIGGRAPH</span> 2001. More recently
deep learning got applied to this problem in Leon Gatys paper &#8220;A Neural Algorithm of Artistic Style&#8221; which has led to a
resurgence of work in this area. There are a few very good tutorials on this subject which the author will turn you towards for
implementing Gatys&#8217; style transfer algorithm. Here are a&nbsp;couple</p>
<p>• Harish Narayanan&#8217;s Convolutional neural networks for artistic style transfer, or
• LogO - Implementing of a Neural Algorithm of Artistic&nbsp;Style</p>
<p>Both of these blogs explain Gatys&#8217; paper and my blog entry is about understanding the nitty gritty of these style transfer algorithms in the context of&nbsp;tensorflow.</p>
<h2>High Level&nbsp;Problem</h2>
<p>Style Transfer is applying the style from one image to the content of&nbsp;another.</p>
<figure>
    <img src="/images/technical/style_xfr1_20250315.png" alt="Original Image">
    <figcaption>Original Image</figcaption>
</figure>

<figure>
    <img src="/images/technical/style_xfr2_20250315.png" alt="Style Image">
    <figcaption>Style Image</figcaption>
</figure>

<figure>
    <img src="/images/technical/style_xfr3_20250315.png" alt="Resultant Image">
    <figcaption>Resultant Image</figcaption>
</figure>

<h2>Prior&nbsp;Knowledge</h2>
<p><span class="caps">VGG</span>-19 is a 19 layer neural network created by Karen Simonyan and Andrew Zisserman (Very Deep Convolutional Networks for
Large-Scale Image Recognition). The neural networks takes as an input an image of size 224 x 224 × 3 and outputs a
classification vector that can classify the image as one of a thousand different&nbsp;objects.</p>
<figure>
    <img src="/images/technical/style_xfr4_20250315.png" alt="Resultant Image">
    <figcaption></figcaption>
</figure>

<p>So, if we feed an image Ai to this network then&nbsp;the</p>
<ul>
<li>the output will be a classification of the image e.g. whether it is a house or a&nbsp;train</li>
<li>the activation at each of hidden layers (blue) are L1(Ai), L2(Ai).. And L19(Ai). The activation at each layer represents a feature
of the image that the network has&nbsp;learnt.</li>
</ul>
<h2>Idea</h2>
<p>So, if the distance between activations from two images A1 and A2 is small ((L1(A1), L2(A1), L3(A1)&#8230; L19(A1)), L1(A2), L2(A2), L3(A2)&#8230;
L19(A2))) then we know that the two images are similar (and probably the&nbsp;same).</p>
<p>Similarly if the distance between the gram matrices of the activations from A1 and A2 is small then the two images are similar in
style. For now assume that this is&nbsp;so.</p>
<p>So, we define two costs which capture content similarity and style similarity and setup an optimization problem to minimize the
two. When the cost is minimized, we have generated an image that has the content from the first and the style from the 2nd.
Note that we wont focus too much on explaining this part since it has been explained well in the Gatys&nbsp;paper.</p>
<h2>The Genius is in the&nbsp;Details</h2>
<p>For the rest of this post, I will assume that you get the code from logO&#8217;s implementation and code pieces refer to the art.py file
from his&nbsp;work.</p>
<h2>If the Neural Network takes a 224x224x3 input, then how can we use it to input an image that is&nbsp;600x800.</h2>
<p>In a <span class="caps">CNN</span> (specifically <span class="caps">VGG</span>-19) there are a few different types of layers convolution_2D, relu, maxpool and fully connected (<span class="caps">FC</span>). If
the <span class="caps">VGG</span>-19 is modified to handle a 600x800 input&nbsp;then.</p>
<ul>
<li>
<p>In the layers of the network prior to <span class="caps">FC</span> layers each layer will be of a different size but convolution filters are scale
independent so the previously learnt weights will continue to work. So for example the first hidden layer after pooling is
112x112x64 in the classic <span class="caps">VGG</span>-19 but in our example it will be 400x300x64. Even though the size of the layer is larger the
same pre-trained weights for the convolution filters will work and activate on same&nbsp;features.</p>
</li>
<li>
<p>The fully connected layers will need different number of weights and retraining, but we will not use that part of the&nbsp;network.</p>
</li>
</ul>
<h2>If the output of the <span class="caps">VGG</span>-19 network is 1x1x1000 then how do we use it to compute an image that is&nbsp;600x800x3?</h2>
<p>We already know that we wont be using the standard <span class="caps">VGG</span>-19 network, specially the fully connected layers. Let me show you
how the optimization problem is set up in such a way that solving for the generated image (see red/blue/green image below)
minimizes the style and content&nbsp;cost.</p>
<h4>Creation of the neural&nbsp;network</h4>
<p>(excerpt from the art.py&nbsp;file)</p>
<div class="highlight"><pre><span></span><code>graph = {}
graphl &#39;input&#39;] = tf. Variable(np.zeros ((1, IMAGE_HEIGHT, IMAGE_WIDTH, COLOR_CHANNELS)),
graph[&#39;conv1_1&#39;] =_conv2d_relu(graph[&#39;input&#39;], 0, &#39;conv1_1&#39;)
graph[&#39;conv1_2&#39;] =_conv2d_relu(graph[&#39;conv1_1&#39;], 2, &#39;conv1_2&#39;)
graph [&#39;avgpool1&#39;] =_avgpool(graph[&#39;conv1_2&#39;])
graph[&#39;conv2_1&#39;] =_conv2d_relu(graph[&#39;avgpool1&#39;], 5, &#39;conv2 _1&#39;)
graph[&#39;conv2_2&#39;] =_conv2d_relu(graph[&#39;conv2_1&#39;], 7, &#39;conv2_2&#39;)
graph[&#39;avgpoo12&#39;] =_avgpool(graph[&#39;conv2_2&#39;])
graphl&#39;conv3_1&#39;] =_conv2d_relu(graphl&#39;avgpool2&#39;], 10, &#39;conv3 _1&#39;)
graph [&#39;conv3_2&#39;] = _conv2d. _relu(graphI&#39;conv3_1&#39;], 12, &#39;conv3 _2&#39;)
graph[&#39;conv3_3&#39;] =_conv2d_relu(graph[&#39;conv3_2&#39;], 14, &#39;conv3_3&#39;)
graph[&#39;conv3_4&#39;] =_conv2d_relu(graph[&#39;conv3_3&#39;], 16, &#39;conv3_4&#39;)
graph [&#39;avgpoo13&#39;] =_avgpool(graph|&#39;conv3_4&#39;])
graph[&#39;conv4_1&#39;] =_conv2d_relu(graph[&#39;avgpool3&#39;], 19, &#39;conv4_1&#39;) &#39;conv4_1&#39;)
graph[&#39;conv4_2&#39;] =_conv2d_relu(graph[&#39;conv4_1&#39;], 21, &#39;conv4_2&#39;)
graph[&#39;conv4_3&#39;] =_conv2d_relu(graph[&#39;conv4_2&#39;], 23, &#39;conv4_3&#39;)
graph[&#39;conv4_4&#39;] =_conv2d_relu(graph[&#39;conv4_3&#39;], 25, &#39;conv4_4&#39;)
graph l&#39;avgpoo14&#39;] =_avgpool(graph[&#39;conv4_4&#39;])
graph[&#39;conv5_1&#39;] =_conv2d_relu(graph[&#39;avgpool4&#39;], 28, &#39;conv5_1&#39;) &#39;conv5_1&#39;)
graphl &#39;conv5_2&#39;] =_conv2d_relu(graph[&#39;conv5_1&#39;], 30, &#39;conv5_2&#39;) &#39;conv5_2&#39;)
graph[&#39;conv5_3&#39;] =_conv2d_relu(graph[&#39;conv5_2&#39;], 32, &#39;conv5_3&#39;) &#39;conv5_3&#39;)
graph[&#39;conv5_4&#39;] =_conv2d_relu(graph[&#39;conv5_3&#39;], 34, &#39;conv5_4&#39;) &quot;conv5_4&#39;)
graph [&#39;avgpool5&#39;] =_avgpool(graph[&#39;conv5_4&#39;])
</code></pre></div>

<p>If you study this code then you&nbsp;learn&#8230;</p>
<ul>
<li>The neural network is setup as a tensorflow computational graph with only one &#8220;variable&#8221; layer which is the first layer.
None of the entries in the rest of the tree are &#8220;variables&#8221;. The output of any of these layers can be computed given the
input but the weights of each of these layers is fixed at this&nbsp;point.</li>
<li>graph[&#8216;input&#8217;] = tf.Variable(np.zeros((1, IMAGE_HEIGHT, IMAGE_WIDTH, COLOR_CHANNELS)), dtype = &#8216;float32&#8217;). The size
of graph[&#8216;input&#8217;] is IMAGE_HEIGHT, IMAGE_WIDTH, COLOR_CHANNELS which can be set to 600 × 800 ×&nbsp;3.</li>
<li>sess.run(graph[&#8216;input&#8217;].assign(content_image). is used to initialize the first layer of the <span class="caps">CNN</span> with an&nbsp;image.</li>
</ul>
<p>In tensorflow variables are the only objects whose values can change when optimization is run. Now let us see how to setup the optimization&nbsp;problem.</p>
<h2>Setting up the Optimization&nbsp;Problem</h2>
<p>Let us assume that there are two functions content_cost and style_cost that we have to minimize, both of them take activations
of two neural networks&#8217; layers as&nbsp;input.</p>
<figure>
    <img src="/images/technical/style_xfr5_20250315.png" alt="Resultant Image">
    <figcaption>Style Transfer Optimization Setup</figcaption>
</figure>

<p>In this set up the neural network is instanciated three times but only the middle one is used for the optimization. In art.py let us
look at the definition of content_loss_func&nbsp;carefully.</p>
<div class="highlight"><pre><span></span><code><span class="n">def</span><span class="w"> </span><span class="n">content_loss_func</span><span class="p">(</span><span class="n">sess</span><span class="p">,</span><span class="w"> </span><span class="n">model</span><span class="p">):</span>
<span class="w">   </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">   Content loss function as defined in the paper.</span>
<span class="sd">   &quot;&quot;&quot;</span>
<span class="w">   </span><span class="n">def</span><span class="w"> </span><span class="n">_content_loss</span><span class="p">(</span><span class="n">p</span><span class="p">,</span><span class="w"> </span><span class="n">x</span><span class="p">):</span>
<span class="w">      </span><span class="c1"># N is the number of filters (at layer l).</span>
<span class="w">      </span><span class="n">N</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">p</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">3</span><span class="p">]</span>
<span class="w">      </span><span class="c1"># M is the height times the width of the feature map (at layer l).</span>
<span class="w">      </span><span class="n">M</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">p</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="w"> </span><span class="o">*</span><span class="w"> </span><span class="n">p</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">2</span><span class="p">]</span>
<span class="w">      </span><span class="c1"># Interestingly, the paper uses this form instead:</span>
<span class="w">      </span><span class="c1">#</span>
<span class="w">      </span><span class="c1"># 0.5 * tf.reduce_sum(tf.pow(x - p, 2))</span>
<span class="w">      </span><span class="c1">#</span>
<span class="w">      </span><span class="c1"># But this form is very slow in &quot;painting&quot; and thus could be missing</span>
<span class="w">      </span><span class="c1"># out some constants (from what I see in other source code), so I&#39;ll</span>
<span class="w">      </span><span class="c1"># replicate the same normalization constant as used in style loss.</span>
<span class="w">      </span><span class="k">return</span><span class="w"> </span><span class="p">(</span><span class="mi">1</span><span class="w"> </span><span class="o">/</span><span class="w"> </span><span class="p">(</span><span class="mi">4</span><span class="w"> </span><span class="o">*</span><span class="w"> </span><span class="n">N</span><span class="w"> </span><span class="o">*</span><span class="w"> </span><span class="n">M</span><span class="p">))</span><span class="w"> </span><span class="o">*</span><span class="w"> </span><span class="n">tf</span><span class="o">.</span><span class="n">reduce_sum</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">pow</span><span class="p">(</span><span class="n">x</span><span class="w"> </span><span class="o">-</span><span class="w"> </span><span class="n">p</span><span class="p">,</span><span class="w"> </span><span class="mi">2</span><span class="p">))</span>
<span class="k">return</span><span class="w"> </span><span class="n">_content_loss</span><span class="p">(</span><span class="n">sess</span><span class="o">.</span><span class="n">run</span><span class="p">(</span><span class="n">model</span><span class="p">[</span><span class="s1">&#39;conv4_2&#39;</span><span class="p">]),</span><span class="w"> </span><span class="n">model</span><span class="p">[</span><span class="s1">&#39;conv4_2&#39;</span><span class="p">])</span>
</code></pre></div>

<p>If you look at the last line of the function then _content_loss is compared between sess.run(model[&#8216;conv4_2&#8217;]) and model[&#8216;conv4_2&#8217;]. And the difference between the first and the second is that the one where the session has been executed, the
value is the instantiated value of the hidden layer (top layer, red). Here the links between the previous layers of the neural
network are no longer valid. So, even though in the code it looks like the model is still active, it is really the values which are
being used. In fact the type of the red layers after sess.run is of type numpy.darray and not tensorflow. Same with the green
squares in the 3rd&nbsp;network.</p>
<p>So, for all intents and purposes the top and bottom neural network may not have existed and we are just optimizing for the only
variable in the graph which is the generated image (see&nbsp;Figure).</p>
<p>To make this further clear, the types of the blue layers is a tensorflow (&#8216;tensorflow.python.framework.ops.Tensor&#8217;) whereas the
type of the red / green layers is a numpy array.&nbsp;(&#8216;numpy.ndarray&#8217;)</p>
<h2>How to Compute a Neural Network only to a certain&nbsp;Point?</h2>
<p>For a newcomer, it is sometimes not clear how to compute a neural network only to a certain point and not all the way to the final
output. So, for example for the figure above the network in blue what if I only want the activations in layer&nbsp;L4.</p>
<p>There are two use cases for&nbsp;this:</p>
<ul>
<li>Connecting up the tensor-flow graph to create a new larger graph which will eventually be used for computation. In this case you will use graph[&#8216;conv2_1&#8217;] to connect up the network. In our example when we constructed the network we had created a dictionary of node names for easy reference. This is how the blue network got connected in this particular&nbsp;example.</li>
<li>Finding the actual value of the activation layer for a specific input. There are two steps in this use&nbsp;case:</li>
</ul>
<div class="highlight"><pre><span></span><code><span class="w">     </span><span class="n">sess</span><span class="p">.</span><span class="n">run</span><span class="p">(</span><span class="n">graph</span><span class="p">[&#39;</span><span class="k">input</span><span class="p">&#39;].</span><span class="k">assign</span><span class="p">(</span><span class="n">content_image</span><span class="p">))</span>
<span class="w">     </span><span class="n">activation_layer</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">sess</span><span class="p">.</span><span class="n">run</span><span class="p">(</span><span class="n">graph</span><span class="p">[&#39;</span><span class="n">conv2_1</span><span class="p">&#39;])</span>
</code></pre></div>

<p>The first line initializes the variable to a particular value (this step can be omitted if the variable already has certain value)
and the second line executes the tensorflow graph to get the value of the layer of type numpy.darray. It is no longer
connected to the rest of the network. This is how the red and the green network got connected in this example. Note that
the bottom layers in green (or the top layers in red) are not connected to each&nbsp;other.</p>
      </div><!-- /.entry-content -->

    </article>
  </section>
                <section id="extras" class="body">
                                <div class="blogroll">
                                        <h2>links</h2>
                                        <ul>
                                                        <li><a href="https://TruU.ai">TruU</a></li>
                                                        <li><a href="https://www.python.org/">Python.org</a></li>
                                                        <li><a href="https://palletsprojects.com/p/jinja/">Jinja2</a></li>
                                                        <li><a href="#">You can modify those links in your config file</a></li>
                                        </ul>
                                </div><!-- /.blogroll -->
                                <div class="social">
                                        <h2>social</h2>
                                        <ul>

                                                        <li><a href="https://www.linkedin.com/in/agnipankh/">Linked-in</a></li>
                                        </ul>
                                </div><!-- /.social -->
                </section><!-- /#extras -->

                <footer id="contentinfo" class="body">
                        <address id="about" class="vcard body">
                                Proudly powered by <a rel="nofollow" href="https://getpelican.com/">Pelican</a>, which takes great advantage of <a rel="nofollow" href="https://www.python.org/">Python</a>.
                        </address><!-- /#about -->

                        <p>The theme is by <a rel="nofollow" href="https://www.smashingmagazine.com/2009/08/designing-a-html-5-layout-from-scratch/">Smashing Magazine</a>, thanks!</p>
                </footer><!-- /#contentinfo -->

        </body>
</html>